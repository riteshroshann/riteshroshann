AI undergrad exploring how learned systems represent and manipulate information. I’m interested in the internal mechanics: how neural networks build functions, encode geometry, and implement algorithms in weight space under the practical limits of data, optimization, and hardware. Architectures are coordinates in a broader design space shaped by symmetry, priors, and compute, not endpoints to be celebrated.

I work bottom-up: train models from scratch, replicate baselines until the curves match, and dissect failure modes across initialization, data pipelines, and kernel execution. The stack matters end-to-end—cryptography and information on top, CUDA and distributed runtimes below, deployment under real latency–throughput–cost constraints at the boundary.

My current work explores models that combine neural representation with geometry, control, and cryptographic structures. Longer term, I’m interested in how notions of representation, generalization, and security interact as we push learning systems toward physical and post-quantum limits.

**Stack:** python, pytorch, jax, cuda/kernels, mpi/nccl, tensor cores, distributed training, model/data/pipe parallelism, tensorRT/onnx, slurm/k8s, linux, hpc, cloud, profiling & optimization, c/c++, rust, ruby on rails, assembly

**_Reverse-engineering how neural networks compute.
**_
