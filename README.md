AI undergrad studying how neural networks implement computation in weight space — how they parameterize functions, encode geometry, and organize latent manifolds under constraints of data, optimization, and hardware. I treat architectures as coordinate systems in a broader design space shaped by symmetry, inductive bias, numerics, and compute.

I work from first principles: training models from scratch, reproducing baselines until dynamics match, and probing the stack from information theory and cryptography down through CUDA/kernels, up to deployment under real latency–throughput–cost constraints.

Current focus: architectures that fuse statistical learning with geometry, control, and post-quantum security — systems that generalize beyond interpolation. Long-term interest in unifying representation, generalization, and security under classical and quantum limits.

**Skills:** representation learning, transformers, scientific ML, geometry, cryptosystems, post-quantum primitives, distributed training, CUDA/kernels, cryptanalysis, HPC, inference stacks.
