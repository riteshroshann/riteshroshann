AI undergrad working at the interface of representation learning, scientific computing, and post-quantum security. I study how neural networks implement computation in weight space — how they encode geometry, structure, and algorithms under constraints of data, optimization, and hardware. Architectures are not endpoints but coordinate systems in a larger design space shaped by symmetry and compute.

I build from first principles: train models from scratch, reproduce baselines until dynamics match, and probe the stack from information theory and cryptography down through CUDA/kernels, up to distributed deployment under real latency–throughput–cost tradeoffs.

Current focus: models that go beyond interpolation by integrating geometry, control, and cryptography. Long-term interest in unifying representation, generalization, and security under both classical and quantum limits.

Mapping the design space of differentiable computation: how architectures encode geometry, algorithms, and security under real constraints.
