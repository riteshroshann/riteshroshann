AI undergrad working at the intersection of representation learning, scientific computing, spatial intelligence, and post-quantum security. I’m interested in how neural networks implement computation in weight space: how they parameterize functions, encode geometry, and organize latent manifolds under the constraints of optimization, data, and compute. This requires treating architectures not as artifacts, but as coordinate systems in a large design space shaped by symmetry, priors, and hardware.

My approach is first-principles and empirical. I train models from scratch, replicate baselines until the loss curves align, and study optimization dynamics across initialization, data pipelines, and kernel-level execution. I move vertically across the stack: from information theory and cryptography, through PyTorch/CUDA/distributed training, up to deployment under latency–throughput–cost constraints.

I’m currently focused on architectures that integrate statistical learning with geometry, control, and physical and cryptographic constraints — systems that generalize beyond interpolation. I believe secure inference and post-quantum threat models will reshape how ML systems are built, monitored, and trusted. The long-term question is how to unify representation, generalization, and security under both classical and quantum limits.

**Skills:** representation learning, transformers, scientific ML, geometry, scaling laws, information theory, cryptosystems, post-quantum primitives, cryptanalysis, CUDA/kernels, distributed training, HPC, scheduling, RL/control, devops, inference stacks.
